
@article{mullensiefen_court_2009,
	title = {Court decisions on music plagiarism and the predictive value of similarity algorithms},
	volume = {13},
	issn = {1029-8649, 2045-4147},
	url = {http://journals.sagepub.com/doi/10.1177/102986490901300111},
	doi = {10.1177/102986490901300111},
	pages = {257--295},
	number = {1},
	journaltitle = {Musicae Scientiae},
	author = {Müllensiefen, Daniel and Pendzich, Marc},
	urldate = {2019-03-20},
	date = {2009-03},
	langid = {english},
	file = {Müllensiefen and Pendzich - 2009 - Court decisions on music plagiarism and the predic.pdf:C\:\\Users\\cristian\\Zotero\\storage\\ST5QNTUE\\Müllensiefen and Pendzich - 2009 - Court decisions on music plagiarism and the predic.pdf:application/pdf}
}

@article{roy_sampling_2017,
	title = {Sampling Variations of Lead Sheets},
	url = {http://arxiv.org/abs/1703.00760},
	abstract = {Machine-learning techniques have been recently used with spectacular results to generate artefacts such as music or text. However, these techniques are still unable to capture and generate artefacts that are convincingly structured. In this paper we present an approach to generate structured musical sequences. We introduce a mechanism for sampling efficiently variations of musical sequences. Given a input sequence and a statistical model, this mechanism samples a set of sequences whose distance to the input sequence is approximately within specified bounds. This mechanism is implemented as an extension of belief propagation, and uses local fields to bias the generation. We show experimentally that sampled sequences are indeed closely correlated to the standard musical similarity measure defined by Mongeau and Sankoff. We then show how this mechanism can used to implement composition strategies that enforce arbitrary structure on a musical lead sheet generation problem.},
	journaltitle = {{arXiv}:1703.00760 [cs]},
	author = {Roy, Pierre and Papadopoulos, Alexandre and Pachet, François},
	urldate = {2019-03-12},
	date = {2017-03-02},
	eprinttype = {arxiv},
	eprint = {1703.00760},
	keywords = {Computer Science - Artificial Intelligence}
}

@inproceedings{brunner_jambot:_2017,
	location = {Boston, {MA}},
	title = {{JamBot}: Music Theory Aware Chord Based Generation of Polyphonic Music with {LSTMs}},
	isbn = {978-1-5386-3876-7},
	url = {https://ieeexplore.ieee.org/document/8371988/},
	doi = {10.1109/ICTAI.2017.00085},
	shorttitle = {{JamBot}},
	eventtitle = {2017 {IEEE} 29th International Conference on Tools with Artificial Intelligence ({ICTAI})},
	pages = {519--526},
	booktitle = {2017 {IEEE} 29th International Conference on Tools with Artificial Intelligence ({ICTAI})},
	publisher = {{IEEE}},
	author = {Brunner, Gino and Wang, Yuyi and Wattenhofer, Roger and Wiesendanger, Jonas},
	urldate = {2019-03-12},
	date = {2017-11}
}

@article{casini_impact_2018,
	title = {The impact of {AI} on the musical world: will musicians be obsolete?},
	volume = {0},
	issn = {1825-8646},
	url = {http://mimesisedizioni.it/journals/index.php/studi-di-estetica/article/view/630},
	shorttitle = {The impact of {AI} on the musical world},
	abstract = {Artificial intelligence ({AI}) is going through a period of renewed interest and suc-cess thanks to the rise of neural networks, the staple of the so-called deep learning. Creating a computer program capable of writing believable music has been tried since the 1960s, with lackluster results. Composing music seemed something beyond the potential of machines, but recent developments in the field are challenging this conception. This article will explore the latest devel-opments falling at the intersection between artificial intelligence and music and then investigate what possible impact such new technologies may have on the musical world, from a technical as well as an aesthetic standpoint, trying to demystify some common misconceptions and worries.},
	number = {12},
	journaltitle = {Studi di estetica},
	author = {Casini, Luca and Roccetti, Marco},
	urldate = {2019-03-12},
	date = {2018-12-01},
	langid = {english},
	keywords = {{AI}, Deep learning, Music},
	file = {Full Text PDF:C\:\\Users\\cristian\\Zotero\\storage\\WVD6CIIW\\Casini and Roccetti - 2018 - The impact of AI on the musical world will musici.pdf:application/pdf;Snapshot:C\:\\Users\\cristian\\Zotero\\storage\\RIT37GJF\\Casini and Roccetti - 2018 - The impact of AI on the musical world will musici.html:text/html}
}

@article{brunner_midi-vae:_2018,
	title = {{MIDI}-{VAE}: Modeling Dynamics and Instrumentation of Music with Applications to Style Transfer},
	url = {http://arxiv.org/abs/1809.07600},
	shorttitle = {{MIDI}-{VAE}},
	abstract = {We introduce {MIDI}-{VAE}, a neural network model based on Variational Autoencoders that is capable of handling polyphonic music with multiple instrument tracks, as well as modeling the dynamics of music by incorporating note durations and velocities. We show that {MIDI}-{VAE} can perform style transfer on symbolic music by automatically changing pitches, dynamics and instruments of a music piece from, e.g., a Classical to a Jazz style. We evaluate the efficacy of the style transfer by training separate style validation classifiers. Our model can also interpolate between short pieces of music, produce medleys and create mixtures of entire songs. The interpolations smoothly change pitches, dynamics and instrumentation to create a harmonic bridge between two music pieces. To the best of our knowledge, this work represents the first successful attempt at applying neural style transfer to complete musical compositions.},
	journaltitle = {{arXiv}:1809.07600 [cs, eess, stat]},
	author = {Brunner, Gino and Konrad, Andres and Wang, Yuyi and Wattenhofer, Roger},
	urldate = {2019-03-12},
	date = {2018-09-20},
	eprinttype = {arxiv},
	eprint = {1809.07600},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, H.5.5, I.2.1, I.2.4, I.2.6, Statistics - Machine Learning}
}

@article{simon_learning_2018,
	title = {Learning a Latent Space of Multitrack Measures},
	url = {https://arxiv.org/abs/1806.00195v1},
	abstract = {Discovering and exploring the underlying structure of multi-instrumental
music using learning-based approaches remains an open problem. We extend the
recent {MusicVAE} model to represent multitrack polyphonic measures as vectors in
a latent space. Our approach enables several useful operations such as
generating plausible measures from scratch, interpolating between measures in a
musically meaningful way, and manipulating specific musical attributes. We also
introduce chord conditioning, which allows all of these operations to be
performed while keeping harmony fixed, and allows chords to be changed while
maintaining musical "style". By generating a sequence of measures over a
predefined chord progression, our model can produce music with convincing
long-term structure. We demonstrate that our latent space model makes it
possible to intuitively control and generate musical sequences with rich
instrumentation (see https://goo.gl/s2N7dV for generated audio).},
	author = {Simon, Ian and Roberts, Adam and Raffel, Colin and Engel, Jesse and Hawthorne, Curtis and Eck, Douglas},
	urldate = {2019-03-12},
	date = {2018-06-01},
	langid = {english},
	file = {Full Text PDF:C\:\\Users\\cristian\\Zotero\\storage\\Z53HJRSX\\Simon et al. - 2018 - Learning a Latent Space of Multitrack Measures.pdf:application/pdf;Snapshot:C\:\\Users\\cristian\\Zotero\\storage\\GHGIA49K\\Simon et al. - 2018 - Learning a Latent Space of Multitrack Measures.html:text/html}
}

@article{lee_music_2011,
	title = {Music Plagiarism Detection System},
	abstract = {This paper proposes a system to detect music plagiarism based on melodic similarity. Melody is obtained using the harmonic structure model, and similarity between two melodies is calculated using the edit distance. The proposed system extracts melody from the input query and finds melodies in the {DB} that are close to the query melody as potential melodies which the query has plagiarized. The proposed system is implemented with a graphical user interface ({GUI}).},
	pages = {3},
	author = {Lee, Juwan and Park, Sanghun and Jo, Seokhwan and Yoo, Chang D},
	date = {2011},
	langid = {english},
	file = {Lee et al. - 2011 - Music Plagiarism Detection System.pdf:C\:\\Users\\cristian\\Zotero\\storage\\THVFUSQZ\\Lee et al. - 2011 - Music Plagiarism Detection System.pdf:application/pdf}
}

@article{yang_evaluation_2018,
	title = {On the evaluation of generative models in music},
	issn = {0941-0643, 1433-3058},
	url = {http://link.springer.com/10.1007/s00521-018-3849-7},
	doi = {10.1007/s00521-018-3849-7},
	abstract = {The modeling of artiﬁcial, human-level creativity is becoming more and more achievable. In recent years, neural networks have been successfully applied to diﬀerent tasks such as image and music generation, demonstrating their great potential in realizing computational creativity. The fuzzy deﬁnition of creativity combined with varying goals of the evaluated generative systems, however, make subjective evaluation seem to be the only viable methodology of choice. We review the evaluation of generative music systems and discuss the inherent challenges of their evaluation. Although subjective evaluation should always be the ultimate choice for the evaluation of creative results, researchers unfamiliar with rigorous subjective experiment design and without the necessary resources for the execution of a large-scale experiment face challenges in terms of reliability, validity, and replicability of the results. In numerous studies, this leads to the report of insigniﬁcant and possibly irrelevant results and the lack of comparability with similar and previous generative systems. Therefore, we propose a set of simple musically informed objective metrics enabling an objective and reproducible way of evaluating and comparing the output of music generative systems. We demonstrate the usefulness of the proposed metrics with several experiments on real-world data.},
	journaltitle = {Neural Computing and Applications},
	author = {Yang, Li-Chia and Lerch, Alexander},
	urldate = {2019-03-20},
	date = {2018-11-03},
	langid = {english},
	file = {Yang and Lerch - 2018 - On the evaluation of generative models in music.pdf:C\:\\Users\\cristian\\Zotero\\storage\\YBTZ9QCC\\Yang and Lerch - 2018 - On the evaluation of generative models in music.pdf:application/pdf}
}

@article{oore_this_2018,
	title = {This Time with Feeling: Learning Expressive Musical Performance},
	url = {http://arxiv.org/abs/1808.03715},
	shorttitle = {This Time with Feeling},
	abstract = {Music generation has generally been focused on either creating scores or interpreting them. We discuss diﬀerences between these two problems and propose that, in fact, it may be valuable to work in the space of direct performance generation: jointly predicting the notes and also their expressive timing and dynamics. We consider the signiﬁcance and qualities of the data set needed for this. Having identiﬁed both a problem domain and characteristics of an appropriate data set, we show an {LSTM}-based recurrent network model that subjectively performs quite well on this task. Critically, we provide generated examples. We also include feedback from professional composers and musicians about some of these examples.},
	journaltitle = {{arXiv}:1808.03715 [cs, eess]},
	author = {Oore, Sageev and Simon, Ian and Dieleman, Sander and Eck, Douglas and Simonyan, Karen},
	urldate = {2019-03-20},
	date = {2018-08-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1808.03715},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Oore et al. - 2018 - This Time with Feeling Learning Expressive Musica.pdf:C\:\\Users\\cristian\\Zotero\\storage\\89NY6SK5\\Oore et al. - 2018 - This Time with Feeling Learning Expressive Musica.pdf:application/pdf}
}

@article{roberts_hierarchical_2018,
	title = {A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music},
	url = {http://arxiv.org/abs/1803.05428},
	abstract = {The Variational Autoencoder ({VAE}) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However, it has thus far seen limited application to sequential data, and, as we demonstrate, existing recurrent {VAE} models have difficulty modeling sequences with long-term structure. To address this issue, we propose the use of a hierarchical decoder, which first outputs embeddings for subsequences of the input and then uses these embeddings to generate each subsequence independently. This structure encourages the model to utilize its latent code, thereby avoiding the "posterior collapse" problem which remains an issue for recurrent {VAEs}. We apply this architecture to modeling sequences of musical notes and find that it exhibits dramatically better sampling, interpolation, and reconstruction performance than a "flat" baseline model. An implementation of our "{MusicVAE}" is available online at http://g.co/magenta/musicvae-code.},
	journaltitle = {{arXiv}:1803.05428 [cs, eess, stat]},
	author = {Roberts, Adam and Engel, Jesse and Raffel, Colin and Hawthorne, Curtis and Eck, Douglas},
	urldate = {2019-03-20},
	date = {2018-03-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1803.05428},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	file = {Roberts et al. - 2018 - A Hierarchical Latent Vector Model for Learning Lo.pdf:C\:\\Users\\cristian\\Zotero\\storage\\B3A36A4I\\Roberts et al. - 2018 - A Hierarchical Latent Vector Model for Learning Lo.pdf:application/pdf}
}

@article{mao_deepj:_2018,
	title = {{DeepJ}: Style-Specific Music Generation},
	url = {http://arxiv.org/abs/1801.00887},
	doi = {10.1109/ICSC.2018.00077},
	shorttitle = {{DeepJ}},
	abstract = {Recent advances in deep neural networks have enabled algorithms to compose music that is comparable to music composed by humans. However, few algorithms allow the user to generate music with tunable parameters. The ability to tune properties of generated music will yield more practical beneﬁts for aiding artists, ﬁlmmakers, and composers in their creative tasks. In this paper, we introduce {DeepJ} - an end-to-end generative model that is capable of composing music conditioned on a speciﬁc mixture of composer styles. Our innovations include methods to learn musical style and music dynamics. We use our model to demonstrate a simple technique for controlling the style of generated music as a proof of concept. Evaluation of our model using human raters shows that we have improved over the Biaxial {LSTM} approach.},
	pages = {377--382},
	journaltitle = {2018 {IEEE} 12th International Conference on Semantic Computing ({ICSC})},
	author = {Mao, Huanru Henry and Shin, Taylor and Cottrell, Garrison W.},
	urldate = {2019-03-20},
	date = {2018-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1801.00887},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Mao et al. - 2018 - DeepJ Style-Specific Music Generation.pdf:C\:\\Users\\cristian\\Zotero\\storage\\BNDGR66J\\Mao et al. - 2018 - DeepJ Style-Specific Music Generation.pdf:application/pdf}
}

@article{huang_music_2018,
	title = {Music Transformer},
	url = {http://arxiv.org/abs/1809.04281},
	abstract = {Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with {ABA} structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions since their memory complexity for intermediate relative information is quadratic in the sequence length. We propose an algorithm that reduces their intermediate memory requirement to linear in the sequence length. This enables us to demonstrate that a Transformer with our modiﬁed relative attention mechanism can generate minutelong compositions (thousands of steps, four times the length modeled in Oore et al. (2018)) with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies1. We evaluate the Transformer with our relative attention mechanism on two datasets, {JSB} Chorales and Piano-e-Competition, and obtain state-of-the-art results on the latter.},
	journaltitle = {{arXiv}:1809.04281 [cs, eess, stat]},
	author = {Huang, Cheng-Zhi Anna and Vaswani, Ashish and Uszkoreit, Jakob and Shazeer, Noam and Simon, Ian and Hawthorne, Curtis and Dai, Andrew M. and Hoffman, Matthew D. and Dinculescu, Monica and Eck, Douglas},
	urldate = {2019-03-20},
	date = {2018-09-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1809.04281},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	file = {Huang et al. - 2018 - Music Transformer.pdf:C\:\\Users\\cristian\\Zotero\\storage\\SKTGEII3\\Huang et al. - 2018 - Music Transformer.pdf:application/pdf}
}

@article{briot_music_2018,
	title = {Music Generation by Deep Learning - Challenges and Directions},
	issn = {0941-0643, 1433-3058},
	url = {http://arxiv.org/abs/1712.04371},
	doi = {10.1007/s00521-018-3813-6},
	abstract = {In addition to traditional tasks such as prediction, classiﬁcation and translation, deep learning is receiving growing attention as an approach for music generation, as witnessed by recent research groups such as Magenta at Google and {CTRL} (Creator Technology Research Lab) at Spotify. The motivation is in using the capacity of deep learning architectures and training techniques to automatically learn musical styles from arbitrary musical corpora and then to generate samples from the estimated distribution. However, a direct application of deep learning to generate content rapidly reaches limits as the generated content tends to mimic the training set without exhibiting true creativity. Moreover, deep learning architectures do not oﬀer direct ways for controlling generation (e.g., imposing some tonality or other arbitrary constraints). Furthermore, deep learning architectures alone are autistic automata which generate music autonomously without human user interaction, far from the objective of interactively assisting musicians to compose and reﬁne music. Issues such as: control, structure, creativity and interactivity are the focus of our analysis. In this paper, we select some limitations of a direct application of deep learning to music generation, analyze why the issues are not fulﬁlled and how to address them by possible approaches. Various examples of recent systems are cited as examples of promising directions.},
	journaltitle = {Neural Computing and Applications},
	author = {Briot, Jean-Pierre and Pachet, François},
	urldate = {2019-03-20},
	date = {2018-10-16},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1712.04371},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Briot and Pachet - 2018 - Music Generation by Deep Learning - Challenges and.pdf:C\:\\Users\\cristian\\Zotero\\storage\\2HZX73TE\\Briot and Pachet - 2018 - Music Generation by Deep Learning - Challenges and.pdf:application/pdf}
}

@article{hadjeres_interactive_2017,
	title = {Interactive Music Generation with Positional Constraints using Anticipation-{RNNs}},
	url = {http://arxiv.org/abs/1709.06404},
	abstract = {Recurrent Neural Networks ({RNNS}) are now widely used on sequence generation tasks due to their ability to learn long-range dependencies and to generate sequences of arbitrary length. However, their left-to-right generation procedure only allows a limited control from a potential user which makes them unsuitable for interactive and creative usages such as interactive music generation.},
	journaltitle = {{arXiv}:1709.06404 [cs, stat]},
	author = {Hadjeres, Gaëtan and Nielsen, Frank},
	urldate = {2019-03-20},
	date = {2017-09-19},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1709.06404},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Hadjeres and Nielsen - 2017 - Interactive Music Generation with Positional Const.pdf:C\:\\Users\\cristian\\Zotero\\storage\\L47MQP23\\Hadjeres and Nielsen - 2017 - Interactive Music Generation with Positional Const.pdf:application/pdf}
}

@article{eck_first_nodate,
	title = {A First Look at Music Composition using {LSTM} Recurrent Neural Networks},
	abstract = {In general music composed by recurrent neural networks ({RNNs}) suﬀers from a lack of global structure. Though networks can learn note-by-note transition probabilities and even reproduce phrases, attempts at learning an entire musical form and using that knowledge to guide composition have been unsuccessful. The reason for this failure seems to be that {RNNs} cannot keep track of temporally distant events that indicate global music structure. Long Short-Term Memory ({LSTM}) has succeeded in similar domains where other {RNNs} have failed, such as timing \& counting and {CSL} learning. In the current study we show that {LSTM} is also a good mechanism for learning to compose music. We compare this approach to previous attempts, with particular focus on issues of data representation. We present experimental results showing that {LSTM} successfully learns a form of blues music and is able to compose novel (and we believe pleasing) melodies in that style. Remarkably, once the network has found the relevant structure it does not drift from it: {LSTM} is able to play the blues with good timing and proper structure as long as one is willing to listen.},
	pages = {11},
	author = {Eck, Douglas and Schmidhuber, Jurgen},
	langid = {english},
	file = {Eck and Schmidhuber - A First Look at Music Composition using LSTM Recur.pdf:C\:\\Users\\cristian\\Zotero\\storage\\RWHC3F7Q\\Eck and Schmidhuber - A First Look at Music Composition using LSTM Recur.pdf:application/pdf}
}